---
---



% ----------

@article{MCU,
  bibtex_show={true},
  title={MCU: Improving Machine Unlearning through Mode Connectivity},
  author={Shi, Yingdan and Wang, Ren},
  journal={arXiv preprint arXiv:2505.10859},
  year={2025},
  html = {https://arxiv.org/pdf/2505.10859},
  preview={MCU.png},
  abbr={unlearning},
  selected={true},
  abstract={Machine Unlearning (MU) aims to remove the information of specific training data from a trained model, ensuring compliance with privacy regulations and user requests. While one line of existing MU methods relies on linear parameter updates via task arithmetic, they suffer from weight entanglement. In this work, we propose a novel MU framework called Mode Connectivity Unlearning (MCU) that leverages mode connectivity to find an unlearning pathway in a nonlinear manner. To further enhance performance and efficiency, we introduce a parameter mask strategy that not only improves unlearning effectiveness but also reduces computational overhead. Moreover, we propose an adaptive adjustment strategy for our unlearning penalty coefficient to adaptively balance forgetting quality and predictive performance during training, eliminating the need for empirical hyperparameter tuning. Unlike traditional MU methods that identify only a single unlearning model, MCU uncovers a spectrum of unlearning models along the pathway. Overall, MCU serves as a plug-and-play framework that seamlessly integrates with any existing MU methods, consistently improving unlearning efficacy. Extensive experiments on the image classification task demonstrate that MCU achieves superior performance.}
}

@article{MUCP,
  bibtex_show={true},
  title={Redefining machine unlearning: A conformal prediction-motivated approach},
  author={Shi, Yingdan and Liu, Sijia and Wang, Ren},
  journal={arXiv preprint arXiv:2501.19403},
  year={2025},
  html = {https://arxiv.org/pdf/2501.19403},
  code={https://github.com/TIML-Group/Conformal-Prediction-Unlearning},
  preview={MUCP.png},
  abbr={unlearning},
  selected={true},
  abstract={Machine unlearning seeks to remove the influence of specified data from a trained model. While metrics such as unlearning accuracy (UA) and membership inference attack (MIA) provide baselines for assessing unlearning performance, they fall short of evaluating the forgetting reliability. In this paper, we find that the data misclassified across UA and MIA still have their ground truth labels included in the prediction set from the uncertainty quantification perspective, which raises a fake unlearning issue. To address this issue, we propose two novel metrics inspired by conformal prediction that more reliably evaluate forgetting quality. Building on these insights, we further propose a conformal prediction-based unlearning framework that integrates conformal prediction into Carlini & Wagner adversarial attack loss, which can significantly push the ground truth label out of the conformal prediction set. Through extensive experiments on image classification task, we demonstrate both the effectiveness of our proposed metrics and the superiority of our unlearning framework, which improves the UA of existing unlearning methods by an average of 6.6% through the incorporation of a tailored loss term alone.
}
}


@article{INS_2025,
  bibtex_show={true},
  title = {Influence Contribution Ratio Estimation in Social Networks},
  journal = {Information Sciences},
  volume = {689},
  pages = {121934},
  year = {2025},
  issn = {0020-0255},
  doi = {https://doi.org/10.1016/j.ins.2025.121934},
  url = {https://www.sciencedirect.com/science/article/pii/S0020025525000660},
  author = {Yingdan Shi and Jingya Zhou and Congcong Zhang and Zhenyu Hu},
  keywords = {Influence estimation, social networks, influence contribution ratio, influence maximization},
  preview={INS_2025.png},
  abbr={social network},
  selected={false}
}


@article{INF_2025,
  bibtex_show={true},
  title = {Order-sensitive competitive revenue maximization for viral marketing in social networks},
  journal = {Information Sciences},
  volume = {689},
  pages = {121474},
  year = {2025},
  issn = {0020-0255},
  doi = {https://doi.org/10.1016/j.ins.2024.121474},
  url = {https://www.sciencedirect.com/science/article/pii/S0020025524013884},
  author = {Congcong Zhang and Jingya Zhou and Wenqi Wei and Yingdan Shi},
  keywords = {Viral marketing, Competitive influence maximization, Competitive revenue maximization, Order-sensitive, Influence diffusion},
  preview={INF_2025.png},
  abbr={social network},
}

@article{ESWA_2024,
  bibtex_show={true},
  title = {Predicting cross-domain collaboration using multi-task learning},
  journal = {Expert Systems with Applications},
  volume = {255},
  pages = {124570},
  year = {2024},
  issn = {0957-4174},
  doi = {https://doi.org/10.1016/j.eswa.2024.124570},
  url = {https://www.sciencedirect.com/science/article/pii/S0957417424014374},
  author = {Zhenyu Hu and Jingya Zhou and Wenqi Wei and Congcong Zhang and Yingdan Shi},
  keywords = {Collaboration prediction, Cross-domain collaboration, Graph representation learning, Multi-task learning},
  preview={ESWA_2024.png},
  abbr={social network},
}

@ARTICLE{TCSS_2024,
  bibtex_show={true},
  author={Zhang, Congcong and Zhou, Jingya and Wang, Jin and Fan, Jianxi and Shi, Yingdan},
  journal={IEEE Transactions on Computational Social Systems}, 
  title={Fairness-Aware Competitive Bidding Influence Maximization in Social Networks}, 
  year={2024},
  volume={11},
  number={2},
  pages={2147-2159},
  keywords={Social networking (online);Resource management;Companies;Blogs;Task analysis;Costs;Reinforcement learning;Competitive bidding influence maximization (CBIM);fairness;influence diffusion;multiagent reinforcement learning (MARL);social networks},
  doi={10.1109/TCSS.2023.3285605},
  preview={TCSS_2024.png},
  abbr={social network},
  }

@INPROCEEDINGS{CSCWD_2024,
  bibtex_show={true},
  author={Hu, Zhenyu and Zhou, Jingya and Zhang, Congcong and Shi, Yingdan},
  booktitle={2024 27th International Conference on Computer Supported Cooperative Work in Design (CSCWD)}, 
  title={Explainable Cross-Domain Collaborator Recommendation}, 
  year={2024},
  volume={},
  number={},
  pages={3224-3229},
  keywords={Representation learning;Attention mechanisms;Social networking (online);Federated learning;Semantics;Collaboration;Cognition;Cross-domain collaboration;collaborator recommendation;explainable recommendation},
  doi={10.1109/CSCWD61410.2024.10580356},
  preview={CSCWD_2024.png},
  abbr={social network},
  }


@article{ESWA_2023,
  bibtex_show={true},
  title = {DySuse: Susceptibility estimation in dynamic social networks},
  journal = {Expert Systems with Applications},
  volume = {234},
  pages = {121042},
  year = {2023},
  issn = {0957-4174},
  doi = {https://doi.org/10.1016/j.eswa.2023.121042},
  url = {https://www.sciencedirect.com/science/article/pii/S0957417423015440},
  author = {Yingdan Shi and Jingya Zhou and Congcong Zhang},
  keywords = {Susceptibility estimation, Dynamic social networks, Influence diffusion, Progressive mechanism},
  preview={ESWA_2023.png},
  abbr={social network},
  selected={false}
}







